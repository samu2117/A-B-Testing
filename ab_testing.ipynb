{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Instructions\n",
    "## Experiment Overview: Free Trial Screener\n",
    "At the time of this experiment, Udacity courses currently have two options on the course overview page: \"start free trial\", and \"access course materials\". If the student clicks \"start free trial\", they will be asked to enter their credit card information, and then they will be enrolled in a free trial for the paid version of the course. After 14 days, they will automatically be charged unless they cancel first. If the student clicks \"access course materials\", they will be able to view the videos and take the quizzes for free, but they will not receive coaching support or a verified certificate, and they will not submit their final project for feedback.\n",
    "\n",
    "\n",
    "In the experiment, Udacity tested a change where if the student clicked \"start free trial\", they were asked how much time they had available to devote to the course. If the student indicated 5 or more hours per week, they would be taken through the checkout process as usual. If they indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free. At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead.\n",
    "\n",
    "\n",
    "The hypothesis was that this might set clearer expectations for students upfront, thus reducing the number of frustrated students who left the free trial because they didn't have enough timeâ€”without significantly reducing the number of students to continue past the free trial and eventually complete the course. If this hypothesis held true, Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.\n",
    "\n",
    "\n",
    "The unit of diversion is a cookie, although if the student enrolls in the free trial, they are tracked by user-id from that point forward. The same user-id cannot enroll in the free trial twice. For users that do not enroll, their user-id is not tracked in the experiment, even if they were signed in when they visited the course overview page.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Metric Choice\n",
    "Which of the following metrics would you choose to measure for this experiment and why? For each metric you choose, indicate whether you would use it as an invariant metric or an evaluation metric. The practical significance boundary for each metric, that is, the difference that would have to be observed before that was a meaningful change for the business, is given in parentheses. All practical significance boundaries are given as absolute changes.\n",
    "\n",
    "\n",
    "Any place \"unique cookies\" are mentioned, the uniqueness is determined by day. (That is, the same cookie visiting on different days would be counted twice.) User-ids are automatically unique since the site does not allow the same user-id to enroll twice.\n",
    "\n",
    "\n",
    "* Number of cookies: That is, number of unique cookies to view the course overview page. (dmin=3000)\n",
    "* Number of user-ids: That is, number of users who enroll in the free trial. (dmin=50)\n",
    "* Number of clicks: That is, number of unique cookies to click the \"Start free trial\" button (which happens before the free trial screener is trigger). (dmin=240)\n",
    "* Click-through-probability: That is, number of unique cookies to click the \"Start free trial\" button divided by number of unique cookies to view the course overview page. (dmin=0.01)\n",
    "* Gross conversion: That is, number of user-ids to complete checkout and enroll in the free trial divided by number of unique cookies to click the \"Start free trial\" button. (dmin= 0.01)\n",
    "* Retention: That is, number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by number of user-ids to complete checkout. (dmin=0.01)\n",
    "* Net conversion: That is, number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of unique cookies to click the \"Start free trial\" button. (dmin= 0.0075)\n",
    "\n",
    "You should also decide now what results you will be looking for in order to launch the experiment. Would a change in any one of your evaluation metrics be sufficient? Would you want to see multiple metrics all move or not move at the same time in order to launch? This decision will inform your choices while designing the experiment.\n",
    "\n",
    "\n",
    "#### Invariant metrics: The following variables marked as invariant and should not change across groups in the experiment. \n",
    "* Number of cookies: This is the unit of diversion, since the user_id is greater than a cookie and the user_id is not stored for the users that are not enrolled to courses. Cookies will be randomly assigned to control or experiment group and both groups should have roughly the same amount number of cookies.\n",
    "* Number of clicks: The number of clicks to the \"Start free trial\" button should also remain constant since the behavior of the button should not change in the experiment, since it is located before the changes we want to measure.\n",
    "* Click-through-probability: The probability that a user clicks the \"Start free trial\" button should be the same in two groups for the same reason that the number of clicks should remain similar.\n",
    "\n",
    "\n",
    "#### Evaluation metrics: Metrics that will be used to assess changes across the control and experiment groups. The metrics that should be chosen for evaluation are the ones that are expected to have an impact in the experiment group.\n",
    "* Gross conversion: The total number of users that enroll in the free trial should be reduced since the message that is being shown in the experiment group should make less dedicated students to retract the enrolling decision in some cases. The total number is being normalized by the number of cookies that click the \"Free trial\" button\n",
    "* Retention: The users that keep enrolled after the 14 day trial divided by the total number of users that started the free trial es expected to increase since it is expected that the more dedicated users are the ones that actually use the free trial.\n",
    "* Net conversion: The net conversion, that is the number of users (ids) that remain enrolled and make a payment may be increase since we are expecting more dedicated users to be enrolled in the course, those users have more probability to keep being enrolled after the free trial. This metric is more likely to increase if the retention increases since the probability of enrollment further the 14 trial is even higher .Again this metric is being normalized by the number of cookies that click the \"Free trial\" button.\n",
    "\n",
    "\n",
    "#### Number of user-ids:\n",
    "* This variable will not be used in this experiment as there is no expectancy of this metric to be constant across experiments and it does not give us enough information as out other metrics have to be an evaluation metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Measuring Variability\n",
    "This spreadsheet contains rough estimates of the baseline values for these metrics (again, these numbers have been changed from Udacity's true numbers).\n",
    "\n",
    "\n",
    "For each metric you selected as an evaluation metric, estimate its standard deviation analytically. Do you expect the analytic estimates to be accurate? That is, for which metrics, if any, would you want to collect an empirical estimate of the variability if you had time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>baseline_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unique cookies to view course overview page pe...</td>\n",
       "      <td>40000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unique cookies to click \"Start free trial\" per...</td>\n",
       "      <td>3200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enrollments per day:</td>\n",
       "      <td>660.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Click-through-probability on \"Start free trial\":</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Probability of enrolling, given click:</td>\n",
       "      <td>0.206250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probability of payment, given enroll:</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Probability of payment, given click</td>\n",
       "      <td>0.109313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              metric  baseline_value\n",
       "0  Unique cookies to view course overview page pe...    40000.000000\n",
       "1  Unique cookies to click \"Start free trial\" per...     3200.000000\n",
       "2                               Enrollments per day:      660.000000\n",
       "3   Click-through-probability on \"Start free trial\":        0.080000\n",
       "4             Probability of enrolling, given click:        0.206250\n",
       "5              Probability of payment, given enroll:        0.530000\n",
       "6                Probability of payment, given click        0.109313"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data.csv\", index_col=False,header = None, names = ['metric','baseline_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variability with sample size 4000: 0.022618503038165455\n",
      "Variability with sample size 8000: 0.015993696878575322\n",
      "Variability with sample size 12000: 0.013058798817751193\n",
      "Variability with sample size 16000: 0.011309251519082728\n",
      "Variability with sample size 20000: 0.010115302068524696\n",
      "Variability with sample size 24000: 0.009233965198182738\n",
      "Variability with sample size 28000: 0.008548990581077812\n",
      "Variability with sample size 32000: 0.007996848439287661\n",
      "Variability with sample size 36000: 0.007539501012721819\n"
     ]
    }
   ],
   "source": [
    "# Gross conversion\n",
    "f = 0.1 #sample size\n",
    "for i in range(1, 10):\n",
    "    s = f*i # fraction of the sample that is being used\n",
    "    p = 0.206250 # probability of enrollment given click\n",
    "    n = 3200*s # cookies that click button times the sample size that is being used\n",
    "    v = np.sqrt((p*(1-p))*(1/n))\n",
    "    print(f\"Variability with sample size {int(s*40000)}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variability with sample size 4000: 0.06143486326380506\n",
      "Variability with sample size 8000: 0.04344100841510487\n",
      "Variability with sample size 12000: 0.035469434842985696\n",
      "Variability with sample size 16000: 0.03071743163190253\n",
      "Variability with sample size 20000: 0.02747450608925454\n",
      "Variability with sample size 24000: 0.025080677902329592\n",
      "Variability with sample size 28000: 0.02322019571789801\n",
      "Variability with sample size 32000: 0.021720504207552435\n",
      "Variability with sample size 36000: 0.020478287754601684\n"
     ]
    }
   ],
   "source": [
    "# Retention\n",
    "f = 0.1 #sample size\n",
    "for i in range(1, 10):\n",
    "    s = f*i # fraction of the sample that is being used\n",
    "    p = 0.530000 # probability of payment given enroll\n",
    "    n = 660*s # users enrolled\n",
    "    v = np.sqrt((p*(1-p))*(1/n))\n",
    "    print(f\"Variability with sample size {int(s*40000)}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variability with sample size 4000: 0.017443092116848864\n",
      "Variability with sample size 8000: 0.012334128720685442\n",
      "Variability with sample size 12000: 0.010070773929162129\n",
      "Variability with sample size 16000: 0.008721546058424432\n",
      "Variability with sample size 20000: 0.0078007879422129535\n",
      "Variability with sample size 24000: 0.0071211125371072334\n",
      "Variability with sample size 28000: 0.006592869119596187\n",
      "Variability with sample size 32000: 0.006167064360342721\n",
      "Variability with sample size 36000: 0.005814364038949621\n"
     ]
    }
   ],
   "source": [
    "# Net conversion\n",
    "f = 0.1 #sample size\n",
    "for i in range(1, 10):\n",
    "    s = f*i # fraction of the sample that is being used\n",
    "    p = 0.109313 # probability of payment given click\n",
    "    n = 3200*s # unique cookies per day\n",
    "    v = np.sqrt((p*(1-p))*(1/n))\n",
    "    print(f\"Variability with sample size {int(s*40000)}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One expected behaviour that we see when we perform the variability for different sample sizes is that the variance decreases are more data is used in the experiment, the variability is proportional to the square root of N.\n",
    "\n",
    "The only metric which will be interesting to assess the empirical variability is the Retention, since the unit of analysis (user_id) is not the same as the unit of diversion (cookie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sizing\n",
    "### Choosing Number of Samples given Power\n",
    "Using the analytic estimates of variance, how many pageviews total (across both groups) would you need to collect to adequately power the experiment? Use an alpha of 0.05 and a beta of 0.2. Make sure you have enough power for each metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following data power will be computed using an online calculator (https://www.evanmiller.org/ab-testing/sample-size.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples needed for Gross conversion: 25835\n",
      "A total of 645875 page_views needs to be taken.\n"
     ]
    }
   ],
   "source": [
    "# Gross conversion\n",
    "p = 0.206250 # probability of enrollment given click\n",
    "b = 0.2\n",
    "a = 0.05\n",
    "d_min = 0.01\n",
    "samples = 25835 # from online calculator\n",
    "print(f\"Number of samples needed for Gross conversion: {samples}\")\n",
    "\n",
    "groups = 2\n",
    "click_per_view = 3200/40000 # 3200 clicks every 40000 pageviews\n",
    "page_views = samples/click_per_view # total page views needed\n",
    "total_views = groups*page_views\n",
    "print(f\"A total of {int(total_views)} page_views needs to be taken.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples needed for Retention: 39115\n",
      "A total of 4741212 page_views needs to be taken.\n"
     ]
    }
   ],
   "source": [
    "# Retention\n",
    "p = 0.530000 # probability of payment given enroll\n",
    "b = 0.2\n",
    "a = 0.05\n",
    "d_min = 0.01\n",
    "samples = 39115 # from online calculator\n",
    "print(f\"Number of samples needed for Retention: {samples}\")\n",
    "\n",
    "groups = 2\n",
    "click_per_view = 660/40000 # 660 enrollments every 40000 pageviews\n",
    "page_views = samples/click_per_view\n",
    "total_views = groups*page_views\n",
    "print(f\"A total of {int(total_views)} page_views needs to be taken.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples needed for Net conversion: 27413\n",
      "A total of 685325 page_views needs to be taken.\n"
     ]
    }
   ],
   "source": [
    "# Net conversion\n",
    "p = 0.109313 # probability of payment given click\n",
    "b = 0.2\n",
    "a = 0.05\n",
    "d_min =  0.0075\n",
    "samples = 27413 # from online calculator\n",
    "print(f\"Number of samples needed for Net conversion: {samples}\")\n",
    "\n",
    "groups = 2\n",
    "click_per_view = 3200/40000 # 3200 clicks every 40000 views\n",
    "page_views = samples/click_per_view\n",
    "total_views = groups*page_views\n",
    "print(f\"A total of {int(total_views)} page_views needs to be taken.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the experiment to work for all three cases, the total number of pagevies that need to be accounted for the experiment are: 4741212 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Choosing Duration vs. Exposure\n",
    "What percentage of Udacity's traffic would you divert to this experiment (assuming there were no other experiments you wanted to run simultaneously)? Is the change risky enough that you wouldn't want to run on all traffic?\n",
    "\n",
    "\n",
    "Given the percentage you chose, how long would the experiment take to run, using the analytic estimates of variance? If the answer is longer than a few weeks, then this is unreasonably long, and you should reconsider an earlier decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change is considerable risky since it may change the probability of enrollment, meaning changes in the revenue of the company. Technical risk are not asumed high since the only changes are an \"alert\" modal with a redirect button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days to measure Retention: 119\n",
      "Days to measure Gross conversion: 18\n",
      "Days to measure Net conversion: 17\n"
     ]
    }
   ],
   "source": [
    "# 100% traffic\n",
    "total_days_retention = 4741212/40000\n",
    "total_days_gross = 685325/40000\n",
    "total_days_net = 645875/40000\n",
    "\n",
    "print(f\"Days to measure Retention: {int(total_days_retention)+1}\")\n",
    "print(f\"Days to measure Gross conversion: {int(total_days_gross)+1}\")\n",
    "print(f\"Days to measure Net conversion: {int(total_days_net)+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days to measure Retention: 149\n",
      "Days to measure Gross conversion: 22\n",
      "Days to measure Net conversion: 21\n"
     ]
    }
   ],
   "source": [
    "# Limiting to 80% traffic\n",
    "traffic = 0.8\n",
    "total_days_retention = 4741212/(40000*traffic)\n",
    "total_days_gross = 685325/(40000*traffic)\n",
    "total_days_net = 645875/(40000*traffic)\n",
    "\n",
    "print(f\"Days to measure Retention: {int(total_days_retention)+1}\")\n",
    "print(f\"Days to measure Gross conversion: {int(total_days_gross)+1}\")\n",
    "print(f\"Days to measure Net conversion: {int(total_days_net)+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 100% of the traffic in the experiment will need 119 days to accout for all the pageviews for the three metrics that we are assessing. Since there is a big difference between the pageviews needed for the conversion and retention metrics, it may be a good idea to drop the retention metric. The sorter term coversion metrics only need 18 days to run the experiment on 100% of the traffic but because it is not recommended, because the experiment may be risky and also to allow room for more experiments to be performed, only the 80% of the traffic will be used. A total of 22 days will be needed to collect all the pageviews for our test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "The data for you to analyze is here. This data contains the raw information needed to compute the above metrics, broken down day by day. Note that there are two sheets within the spreadsheet - one for the experiment group, and one for the control group.\n",
    "\n",
    "\n",
    "The meaning of each column is:\n",
    "\n",
    "* Pageviews: Number of unique cookies to view the course overview page that day.\n",
    "* Clicks: Number of unique cookies to click the course overview page that day.\n",
    "* Enrollments: Number of user-ids to enroll in the free trial that day.\n",
    "* Payments: Number of user-ids who who enrolled on that day to remain enrolled for 14 days and thus make a payment. (Note that the date for this column is the start date, that is, the date of enrollment, rather than the date of the payment. The payment happened 14 days later. Because of this, the enrollments and payments are tracked for 14 fewer days than the other columns.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date  Pageviews  Clicks  Enrollments  Payments\n",
      "0  Sat, Oct 11       7723     687        134.0      70.0\n",
      "1  Sun, Oct 12       9102     779        147.0      70.0\n",
      "2  Mon, Oct 13      10511     909        167.0      95.0\n",
      "3  Tue, Oct 14       9871     836        156.0     105.0\n",
      "4  Wed, Oct 15      10014     837        163.0      64.0\n",
      "           Date  Pageviews  Clicks  Enrollments  Payments\n",
      "32  Wed, Nov 12      10134     801          NaN       NaN\n",
      "33  Thu, Nov 13       9717     814          NaN       NaN\n",
      "34  Fri, Nov 14       9192     735          NaN       NaN\n",
      "35  Sat, Nov 15       8630     743          NaN       NaN\n",
      "36  Sun, Nov 16       8970     722          NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "# Control Data\n",
    "df_control = pd.read_csv(\"control_data.csv\")\n",
    "print(df_control.head())\n",
    "print(df_control.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date  Pageviews  Clicks  Enrollments  Payments\n",
      "0  Sat, Oct 11       7716     686        105.0      34.0\n",
      "1  Sun, Oct 12       9288     785        116.0      91.0\n",
      "2  Mon, Oct 13      10480     884        145.0      79.0\n",
      "3  Tue, Oct 14       9867     827        138.0      92.0\n",
      "4  Wed, Oct 15       9793     832        140.0      94.0\n",
      "           Date  Pageviews  Clicks  Enrollments  Payments\n",
      "32  Wed, Nov 12      10042     802          NaN       NaN\n",
      "33  Thu, Nov 13       9721     829          NaN       NaN\n",
      "34  Fri, Nov 14       9304     770          NaN       NaN\n",
      "35  Sat, Nov 15       8668     724          NaN       NaN\n",
      "36  Sun, Nov 16       8988     710          NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "# Experiment data\n",
    "df_experiment = pd.read_csv(\"experiment_data.csv\")\n",
    "print(df_experiment.head())\n",
    "print(df_experiment.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks\n",
    "Start by checking whether your invariant metrics are equivalent between the two groups. If the invariant metric is a simple count that should be randomly split between the 2 groups, you can use a binomial test as demonstrated in Lesson 5. Otherwise, you will need to construct a confidence interval for a difference in proportions using a similar strategy as in Lesson 1, then check whether the difference between group values falls within that confidence level.\n",
    "\n",
    "\n",
    "If your sanity checks fail, look at the day by day data and see if you can offer any insight into what is causing the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(control, experiment, p=0.5):\n",
    "    # Number of Counts\n",
    "    count_control = sum(control)\n",
    "    print(f\"Count in control group: {count_control}\")\n",
    "    count_experiment = sum(experiment)\n",
    "    print(f\"Count in experiment group: {count_experiment}\")\n",
    "\n",
    "    # Standard Error\n",
    "    std_error = np.sqrt((p * (1 - p))/(count_control+count_experiment))\n",
    "    print(f\"Standard error: {round(std_error, 6)}\")\n",
    "\n",
    "    # Margin of error\n",
    "    mrg = 1.96 * std_error\n",
    "    print(f\"Margin of error: {round(mrg, 6)}\")\n",
    "\n",
    "    # Confidence Interval \n",
    "    ci_low = p - mrg\n",
    "    ci_up = p + mrg\n",
    "    print(f\"Confidence Interval: Lower bound = {round(ci_low, 6)}\")\n",
    "    print(f\"Confidence Interval: Upper bound = {round(ci_up, 6)}\")\n",
    "\n",
    "    # Observed experiment probability\n",
    "    obs_p = count_experiment/(count_experiment+count_control)\n",
    "    print(f\"Observed probability: {round(obs_p, 6)}\")\n",
    "\n",
    "    # Sanity pass\n",
    "    if obs_p > ci_low and obs_p < ci_up:\n",
    "        print(f\"Sanity check passed! ({round(obs_p, 6)} in [{round(ci_low, 6)},{round(ci_up, 6)}])\")\n",
    "    else:\n",
    "        print(f\"Sanity check not passed! ({round(obs_p, 6)} not in [{round(ci_low, 6)},{round(ci_up, 6)}])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count in control group: 345543\n",
      "Count in experiment group: 344660\n",
      "Standard error: 0.000602\n",
      "Margin of error: 0.00118\n",
      "Confidence Interval: Lower bound = 0.49882\n",
      "Confidence Interval: Upper bound = 0.50118\n",
      "Observed probability: 0.49936\n",
      "Sanity check passed! (0.49936 in [0.49882,0.50118])\n"
     ]
    }
   ],
   "source": [
    "# Number of cookies\n",
    "sanity_check(df_control[\"Pageviews\"], df_experiment[\"Pageviews\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count in control group: 28378\n",
      "Count in experiment group: 28325\n",
      "Standard error: 0.0021\n",
      "Margin of error: 0.004116\n",
      "Confidence Interval: Lower bound = 0.495884\n",
      "Confidence Interval: Upper bound = 0.504116\n",
      "Observed probability: 0.499533\n",
      "Sanity check passed! (0.499533 in [0.495884,0.504116])\n"
     ]
    }
   ],
   "source": [
    "# Number of clicks\n",
    "sanity_check(df_control[\"Clicks\"], df_experiment[\"Clicks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled probability: 0.082154\n",
      "Standard error: 0.000331\n",
      "Margin of error: 0.000648\n",
      "Confidence Interval: Lower bound = 0.081506\n",
      "Confidence Interval: Upper bound = 0.082802\n",
      "Observed probability: 0.082182\n",
      "Sanity check passed! (0.082182 in [0.081506,0.082802])\n"
     ]
    }
   ],
   "source": [
    "# Click-through probability\n",
    "p_control = sum(df_control[\"Clicks\"])/sum(df_control[\"Pageviews\"])\n",
    "p_experiment = sum(df_experiment[\"Clicks\"])/sum(df_experiment[\"Pageviews\"])\n",
    "pooled_prob = (p_control+p_experiment)/2\n",
    "print(f\"Pooled probability: {round(pooled_prob, 6)}\")\n",
    "\n",
    "# Standard Error\n",
    "std_error = np.sqrt((pooled_prob * (1 - pooled_prob))/(sum(df_control[\"Pageviews\"])+sum(df_experiment[\"Pageviews\"])))\n",
    "print(f\"Standard error: {round(std_error, 6)}\")\n",
    "\n",
    "# Margin of error\n",
    "mrg = 1.96 * std_error\n",
    "print(f\"Margin of error: {round(mrg, 6)}\")\n",
    "\n",
    "# Confidence Interval \n",
    "ci_low = pooled_prob - mrg\n",
    "ci_up = pooled_prob + mrg\n",
    "print(f\"Confidence Interval: Lower bound = {round(ci_low, 6)}\")\n",
    "print(f\"Confidence Interval: Upper bound = {round(ci_up, 6)}\")\n",
    "\n",
    "# Observed experiment probability\n",
    "print(f\"Observed probability: {round(p_experiment, 6)}\")\n",
    "\n",
    "# Sanity pass\n",
    "if p_experiment > ci_low and p_experiment < ci_up:\n",
    "    print(f\"Sanity check passed! ({round(p_experiment, 6)} in [{round(ci_low, 6)},{round(ci_up, 6)}])\")\n",
    "else:\n",
    "    print(f\"Sanity check not passed! ({round(p_experiment, 6)} not in [{round(ci_low, 6)},{round(ci_up, 6)}])\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count in control group: 3785.0\n",
      "Count in experiment group: 3423.0\n",
      "Standard error: 0.005889\n",
      "Margin of error: 0.011543\n",
      "Confidence Interval: Lower bound = 0.488457\n",
      "Confidence Interval: Upper bound = 0.511543\n",
      "Observed probability: 0.474889\n",
      "Sanity check not passed! (0.474889 not in [0.488457,0.511543])\n"
     ]
    }
   ],
   "source": [
    "# Enrollments\n",
    "sanity_check(df_control[\"Enrollments\"].dropna(), df_experiment[\"Enrollments\"].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count in control group: 2033.0\n",
      "Count in experiment group: 1945.0\n",
      "Standard error: 0.007928\n",
      "Margin of error: 0.015538\n",
      "Confidence Interval: Lower bound = 0.484462\n",
      "Confidence Interval: Upper bound = 0.515538\n",
      "Observed probability: 0.488939\n",
      "Sanity check passed! (0.488939 in [0.484462,0.515538])\n"
     ]
    }
   ],
   "source": [
    "# Payments\n",
    "sanity_check(df_control[\"Payments\"].dropna(), df_experiment[\"Payments\"].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results seen in the sanity check work in accordance as we can expect, all the metrics pass the sanity check as there is no significant difference between the experiment and control groups. The only metric that shows an statistical significant difference is the \"Enrollments\" metric, which is not a invariant metric but is one metric where we want to see an actual difference. Having less observations in the experiment group means that the expected behaviour of the modal is as intended and reducing the enrollments for less dedicated users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Practical and Statistical Significance\n",
    "Next, for your evaluation metrics, calculate a confidence interval for the difference between the experiment and control groups, and check whether each metric is statistically and/or practically significance. A metric is statistically significant if the confidence interval does not include 0 (that is, you can be confident there was a change), and it is practically significant if the confidence interval does not include the practical significance boundary (that is, you can be confident there is a change that matters to the business.)\n",
    "\n",
    "\n",
    "If you have chosen multiple evaluation metrics, you will need to decide whether to use the Bonferroni correction. When deciding, keep in mind the results you are looking for in order to launch the experiment. Will the fact that you have multiple metrics make those results more likely to occur by chance than the alpha level of 0.05?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove observations that do not contain payment or enrollment info\n",
    "df_control = df_control[pd.notna(df_control[\"Enrollments\"])]\n",
    "df_experiment = df_experiment[pd.notna(df_experiment[\"Enrollments\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_difference(control, experiment, n_control, n_experiment, significance):\n",
    "    p_control = sum(control)/sum(n_control)\n",
    "    p_experiment = sum(experiment)/sum(n_experiment)\n",
    "\n",
    "    p_hat = (sum(control)+sum(experiment))/(sum(n_control)+sum(n_experiment))\n",
    "    print(f\"Pooled probability: {round(p_hat, 6)}\")\n",
    "\n",
    "    # Standard Error\n",
    "    std_error = np.sqrt((p_hat * (1 - p_hat))*(1/sum(n_control)+1/sum(n_experiment)))\n",
    "    # std_error = np.sqrt((p_hat * (1- p_hat ))*(1/sum(n_control) + 1/sum(n_experiment)))\n",
    "    print(f\"Standard error: {round(std_error, 6)}\")\n",
    "\n",
    "    # Margin of error\n",
    "    mrg = 1.96 * std_error\n",
    "    print(f\"Margin of error: {round(mrg, 6)}\")\n",
    "\n",
    "    # Confidence Interval \n",
    "    ci_low = p_experiment - p_control - mrg\n",
    "    ci_up = p_experiment - p_control + mrg\n",
    "    print(f\"Confidence Interval: Lower bound = {round(ci_low, 6)}\")\n",
    "    print(f\"Confidence Interval: Upper bound = {round(ci_up, 6)}\")\n",
    "\n",
    "    print(f\"Difference of experiment and control probabilities: {round(p_experiment-p_control, 6)}\")\n",
    "\n",
    "    if abs(p_experiment - p_control) > significance:\n",
    "        print(f\"Difference is greater thatn the practical significance boundary\")\n",
    "    else:\n",
    "        print(\"The difference is not greater than the practival significance boundary\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled probability: 0.208607\n",
      "Standard error: 0.004372\n",
      "Margin of error: 0.008568\n",
      "Confidence Interval: Lower bound = -0.029123\n",
      "Confidence Interval: Upper bound = -0.011986\n",
      "Difference of experiment and control probabilities: -0.020555\n",
      "Difference is greater thatn the practical significance boundary\n"
     ]
    }
   ],
   "source": [
    "# Gross conversion\n",
    "statistical_difference(df_control[\"Enrollments\"], df_experiment[\"Enrollments\"], df_control[\"Clicks\"], df_experiment[\"Clicks\"], 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled probability: 0.551887\n",
      "Standard error: 0.01173\n",
      "Margin of error: 0.02299\n",
      "Confidence Interval: Lower bound = 0.008104\n",
      "Confidence Interval: Upper bound = 0.054085\n",
      "Difference of experiment and control probabilities: 0.031095\n",
      "Difference is greater thatn the practical significance boundary\n"
     ]
    }
   ],
   "source": [
    "# Retention\n",
    "statistical_difference(df_control[\"Payments\"], df_experiment[\"Payments\"], df_control[\"Enrollments\"], df_experiment[\"Enrollments\"], 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled probability: 0.115127\n",
      "Standard error: 0.003434\n",
      "Margin of error: 0.006731\n",
      "Confidence Interval: Lower bound = -0.011605\n",
      "Confidence Interval: Upper bound = 0.001857\n",
      "Difference of experiment and control probabilities: -0.004874\n",
      "The difference is not greater than the practival significance boundary\n"
     ]
    }
   ],
   "source": [
    "# Net conversion\n",
    "statistical_difference(df_control[\"Payments\"], df_experiment[\"Payments\"], df_control[\"Clicks\"], df_experiment[\"Clicks\"], 0.0075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After evaluating the significance of the metrics and assuming that we were able to collect all the necessary page views, the behavior of the evaluation metrics was the expected one. \n",
    "The Gross conversion metric show an statistical significant difference in the experiment group with respect to the control group, the confidence interval at 95% significance level, does not contain 0 and the difference is greater than the practical significance level. An important outcome of this metric what to contrast if the number of enrolled users would decrease and they in fact did since we observed a 2% difference.\n",
    "The Retention, again given that we had enough power to collect data, also showed an statistical significant difference with respect the control group. We wanted for this metric to increase and it indeed increased with an augment of 3%, again greater than the practical significance level, meaning that the users that enroll to the free trial are more commited to the study and continue enrolled after the 14 day period.\n",
    "The Net conversion did not showed statistical significance difference since the confidence interval contains the 0 value (we are not guaranteed that the difference will always be different than zero 95% percent of the time). The absolute difference is also not greater than the practical significance boundary. This metric not being significative is actually a good sign, although the number of users that enroll the free trial decreases, the number of users that keep enrolled after the 14 day period may be reduced a little but there is no significal difference so the revenue of the company will not be affected significantly and the instructurs will be focused on more commited students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Run Sign Tests\n",
    "For each evaluation metric, do a sign test using the day-by-day breakdown. If the sign test does not agree with the confidence interval for the difference, see if you can figure out why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=35.0, pvalue=0.0009758472442626953)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gross conversion\n",
    "gross_control = df_control[\"Enrollments\"]/df_control[\"Clicks\"]\n",
    "gross_experiment = df_experiment[\"Enrollments\"]/df_experiment[\"Clicks\"]\n",
    "\n",
    "scipy.stats.wilcoxon(gross_control, gross_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=107.0, pvalue=0.36039113998413086)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retention\n",
    "retention_control = df_control[\"Payments\"]/df_control[\"Enrollments\"]\n",
    "retention_experiment = df_experiment[\"Payments\"]/df_experiment[\"Enrollments\"]\n",
    "\n",
    "scipy.stats.wilcoxon(retention_control, retention_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_diff = retention_control-retention_experiment\n",
    "sum(ret_diff > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.198579\n",
       "1    -0.308292\n",
       "2     0.024035\n",
       "3     0.006410\n",
       "4    -0.278791\n",
       "5     0.121335\n",
       "6     0.174091\n",
       "7    -0.023211\n",
       "8    -0.183651\n",
       "9    -0.052644\n",
       "10    0.039211\n",
       "11    0.021026\n",
       "12   -0.116432\n",
       "13    0.070009\n",
       "14    0.089477\n",
       "15   -0.014168\n",
       "16   -0.026497\n",
       "17   -0.149832\n",
       "18   -0.038836\n",
       "19    0.125430\n",
       "20   -0.095364\n",
       "21   -0.108072\n",
       "22   -0.240691\n",
       "dtype: float64"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=120.0, pvalue=0.6010458469390869)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Net conversion\n",
    "net_control = df_control[\"Payments\"]/df_control[\"Clicks\"]\n",
    "net_experiment = df_experiment[\"Payments\"]/df_experiment[\"Clicks\"]\n",
    "\n",
    "scipy.stats.wilcoxon(net_control, net_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the Sign test make us realize that there is not a really significant difference in the Retention metric. Initially we already knew that we needed more power in order to properly run this experiment. The results testing with confidence intervals were significative only by chance. Seeing the high variance that this metric has and being the condifence interval that close to zero was something that had to make us doubt about the significance of this metric.\n",
    "\n",
    "Gross conversion and Net conversion are variables that behave like we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Make a Recommendation\n",
    "Finally, make a recommendation. Would you launch this experiment, not launch it, dig deeper, run a follow-up experiment, or is it a judgment call? If you would dig deeper, explain what area you would investigate. If you would run follow-up experiments, briefIy describe that experiment. If it is a judgment call, explain what factors would be relevant to the decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would recommend to retrieve more data for this experiment since it sounds promessing. The results that we have analyzed with the small number of samples that we have available showed that even if the proportion of enrolled studets decreased, the number of students that keep paying after being enrolled did not decrease, meaning that what we wanted to accomplish with this experiment which was to readuce the number of non-commited students has been accomplished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow-Up Experiment: How to Reduce Early Cancellations\n",
    "If you wanted to reduce the number of frustrated students who cancel early in the course, what experiment would you try? Give a brief description of the change you would make, what your hypothesis would be about the effect of the change, what metrics you would want to measure, and what unit of diversion you would use. Include an explanation of each of your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce early cancellations, first we have to understand what is what cause the cancellation for students. It may be the difficulty of the courses (which can be increased if the students are not really prepared for this course), the performance of the page, maybe the personalized experience (teachers and additional rosources) are not responding correctly. \n",
    "\n",
    "After studying some of the reasons that can make a user not be interested in the subscription and therefore cancelling it, we have proposed the following added feature: Congratulatory message and reward points every daily login.\n",
    "\n",
    "The hypothesis is that this change should decrease the number of early cancellations since the user would be more interested in the education page.\n",
    "\n",
    "In order to study these metrics we can collect the following metrics:\n",
    "* Daily logins (user diversion metric): this metric will help us to measure the daily logins -> It should increase in the experiment in order to validate this change\n",
    "* Number of lessons a student does every day (user diversion metric): this number also can increase if the user will receive more rewards\n",
    "* Average time of lesson (user diversion metric): this metric should not change since the change should not increase the difficulty of the courses\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pilatus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
